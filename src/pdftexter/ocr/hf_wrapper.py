"""
HuggingFace Transformersç‰ˆDeepSeek-OCRãƒ©ãƒƒãƒ‘ãƒ¼
vLLMã‚µãƒ¼ãƒãƒ¼ä¸è¦ã§ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã§ãã‚‹ç°¡å˜ãªæ–¹æ³•
"""

import sys
from pathlib import Path
from typing import Optional

try:
    from transformers import AutoModel, AutoTokenizer
    from PIL import Image
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    torch = None


class HuggingFaceOCRWrapper:
    """HuggingFace Transformersç‰ˆDeepSeek-OCRãƒ©ãƒƒãƒ‘ãƒ¼"""
    
    def __init__(self, model_path: str):
        """
        åˆæœŸåŒ–
        
        Args:
            model_path: DeepSeek-OCRãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ï¼ˆHuggingFaceãƒ¢ãƒ‡ãƒ«IDã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ï¼‰
            
        Raises:
            ImportError: transformersãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆ
            FileNotFoundError: ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        """
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError(
                "transformersãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n"
                "ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:\n"
                "  pip install transformers pillow torch\n"
                "ã¾ãŸã¯:\n"
                "  uv pip install transformers pillow torch"
            )
        
        self.model_path = model_path
        print(f"DeepSeek-OCRãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_path}...", file=sys.stderr)
        
        # Flash Attention 2é–¢é€£ã®ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã™ã‚‹ãŸã‚ã€ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒã‚’é©ç”¨
        # DeepSeek-OCRãƒ¢ãƒ‡ãƒ«ã®ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¼ãƒ‰ãŒLlamaFlashAttention2ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã‚ˆã†ã¨ã™ã‚‹ã®ã‚’é˜²ãã¾ã™
        # LlamaFlashAttention2ã®ãƒ€ãƒŸãƒ¼ã‚¯ãƒ©ã‚¹ã‚’å®šç¾©
        class DummyLlamaFlashAttention2:
            """LlamaFlashAttention2ã®ãƒ€ãƒŸãƒ¼ã‚¯ãƒ©ã‚¹ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰"""
            pass
        
        # transformers.models.llama.modeling_llamaãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ãƒ€ãƒŸãƒ¼ã‚¯ãƒ©ã‚¹ã‚’è¿½åŠ 
        # ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¼ãƒ‰ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ã‚’èµ·ã“ã•ãªã„ã‚ˆã†ã«ã—ã¾ã™
        try:
            import transformers.models.llama.modeling_llama as llama_module
            if not hasattr(llama_module, 'LlamaFlashAttention2'):
                # LlamaFlashAttention2ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ãƒ€ãƒŸãƒ¼ã‚¯ãƒ©ã‚¹ã‚’è¿½åŠ 
                llama_module.LlamaFlashAttention2 = DummyLlamaFlashAttention2
        except ImportError:
            pass
        
        try:
            # Tokenizerã¨ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆå…¬å¼ã®æ¨å¥¨æ–¹æ³•ï¼‰
            # å‚è€ƒ: https://github.com/deepseek-ai/DeepSeek-OCR
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆå…¬å¼ã®æ¨å¥¨æ–¹æ³•ã«å¾“ã†ï¼‰
            # å…¬å¼READMEã§ã¯ _attn_implementation='flash_attention_2' ã‚’æ¨å¥¨
            # ãŸã ã—ã€flash-attnãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            use_flash_attention_2 = False
            try:
                # flash-attnãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                import flash_attn  # noqa: F401
                use_flash_attention_2 = True
            except ImportError:
                # flash-attnãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯æ¨™æº–å®Ÿè£…ã‚’ä½¿ç”¨
                use_flash_attention_2 = False
            
            if use_flash_attention_2:
                # å…¬å¼ã®æ¨å¥¨æ–¹æ³•ï¼šflash_attention_2ã‚’ä½¿ç”¨
                try:
                    self.model = AutoModel.from_pretrained(
                        model_path,
                        _attn_implementation='flash_attention_2',
                        trust_remote_code=True,
                        use_safetensors=True,
                    )
                    print("âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆFlash Attention 2ã‚’ä½¿ç”¨ï¼‰", file=sys.stderr)
                except Exception as e:
                    # flash_attention_2ãŒä½¿ãˆãªã„å ´åˆã¯æ¨™æº–å®Ÿè£…ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    error_msg = str(e)
                    print(f"âš  Flash Attention 2ã®ä½¿ç”¨ã«å¤±æ•—ã—ã¾ã—ãŸ: {error_msg}", file=sys.stderr)
                    print("âš  æ¨™æº–ã®attentionå®Ÿè£…ã‚’ä½¿ç”¨ã—ã¾ã™", file=sys.stderr)
                    self.model = AutoModel.from_pretrained(
                        model_path,
                        trust_remote_code=True,
                        use_safetensors=True,
                    )
                    print("âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆæ¨™æº–ã®attentionå®Ÿè£…ã‚’ä½¿ç”¨ï¼‰", file=sys.stderr)
            else:
                # flash-attnãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯æ¨™æº–å®Ÿè£…ã‚’ä½¿ç”¨
                self.model = AutoModel.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    use_safetensors=True,
                )
                print("âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆæ¨™æº–ã®attentionå®Ÿè£…ã‚’ä½¿ç”¨ï¼‰", file=sys.stderr)
                print("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: flash-attnã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã¨ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ã¾ã™", file=sys.stderr)
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šã—ã€GPUã«ç§»å‹•
            self.model = self.model.eval()
            if torch is not None and torch.cuda.is_available():
                try:
                    self.model = self.model.cuda().to(torch.bfloat16)
                    print("âœ“ GPUã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã—ã¾ã™", file=sys.stderr)
                except Exception:
                    self.model = self.model.cuda()
                    print("âœ“ GPUã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã—ã¾ã™ï¼ˆbfloat16ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ï¼‰", file=sys.stderr)
            else:
                print("âš  GPUãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€CPUã§æ¨è«–ã—ã¾ã™", file=sys.stderr)
            
            print("âœ“ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ", file=sys.stderr)
            
        except Exception as e:
            raise RuntimeError(
                f"ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\n"
                f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {model_path}"
            )
    
    def process_image(
        self,
        image_path: str,
        prompt: str = "<image>\n<|grounding|>Convert the document to markdown.",
    ) -> str:
        """
        ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’OCRå‡¦ç†ã™ã‚‹
        
        Args:
            image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            OCRçµæœã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆMarkdownå½¢å¼ï¼‰
        """
        image_file = Path(image_path)
        if not image_file.exists():
            raise FileNotFoundError(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_path}")
        
        # DeepSeek-OCRã®å…¬å¼å®Ÿè£…ã§ã¯`model.infer()`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        # infer(self, tokenizer, prompt='', image_file='', output_path='', 
        #       base_size=1024, image_size=640, crop_mode=True, 
        #       test_compress=False, save_results=False)
        if hasattr(self.model, 'infer'):
            # å…¬å¼ã®æ¨å¥¨æ–¹æ³•ï¼š`infer`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
            result = self.model.infer(
                self.tokenizer,
                prompt=prompt,
                image_file=str(image_path),
                output_path='',  # çµæœã‚’ä¿å­˜ã—ãªã„
                base_size=1024,
                image_size=640,
                crop_mode=True,
                test_compress=False,
                save_results=False,
            )
            return result
        else:
            raise RuntimeError(
                "DeepSeek-OCRãƒ¢ãƒ‡ãƒ«ã«`infer`ãƒ¡ã‚½ãƒƒãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
                "ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )


# torchã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import torch
except ImportError:
    torch = None

